{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124280,
     "status": "ok",
     "timestamp": 1675601197941,
     "user": {
      "displayName": "Liviu Balan",
      "userId": "06381693624622414377"
     },
     "user_tz": -300
    },
    "id": "pcj4YYCvUgPJ",
    "outputId": "227a49f8-e945-40ec-ec4d-ef5cced236f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1675601197942,
     "user": {
      "displayName": "Liviu Balan",
      "userId": "06381693624622414377"
     },
     "user_tz": -300
    },
    "id": "GN6g5h43Ugri",
    "outputId": "eafaa984-9373-4803-a37e-3c29f2a68256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/drive/MyDrive/Diffusion/StyleT2I\n"
     ]
    }
   ],
   "source": [
    "cd Text-to-Face-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6699,
     "status": "ok",
     "timestamp": 1675601204637,
     "user": {
      "displayName": "Liviu Balan",
      "userId": "06381693624622414377"
     },
     "user_tz": -300
    },
    "id": "EJUXztjCUzLp",
    "outputId": "654a3e03-ff07-4a87-d614-652e47a13900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-bjg8hzbh\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-bjg8hzbh\n",
      "  Resolved https://github.com/openai/CLIP.git to commit 3702849800aa56e2223035bccd1c6ef91c704ca8\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.25.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (4.0.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369408 sha256=b8913e8434d078c39887f4b1c6850ad3f7d616c49624214346c3010dd1bd865d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xwszt3pz/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, clip\n",
      "Successfully installed clip-1.0 ftfy-6.1.1\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSlxhB-kvjQA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from model.utils import differentiable_clip_preprocess_from_stylegan\n",
    "from model.stylegan2.model import Generator\n",
    "from dataset.celebahq import CelebAHQ\n",
    "from torchvision import transforms\n",
    "from dataset.data_utils import pad_text_seq_collate\n",
    "from model.data_utils import sample_data\n",
    "from model.text_encoder_cond import Sentence2DeltaLatent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-MlXXzhvnej"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.device = torch.device(0)\n",
    "\n",
    "        # model\n",
    "        self.generator = Generator(args.stylegan_size, 512, 8)\n",
    "        stylegan_ckpt = torch.load(args.ckpt)\n",
    "        g_ckpt = stylegan_ckpt[\"g_ema\"]\n",
    "        self.generator.load_state_dict(g_ckpt, strict=False)\n",
    "        self.generator.eval()\n",
    "        self.generator = self.generator.to(self.device)\n",
    "        for p in self.generator.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.synthesis_kwargs = dict(input_is_latent=True, randomize_noise=False)\n",
    "\n",
    "        if args.truncation < 1:\n",
    "            self.mean_latent = self.generator.mean_latent(4096)\n",
    "        else:\n",
    "            self.mean_latent = None\n",
    "\n",
    "        self.clip_model_for_train = clip.load(\"ViT-B/32\", device=\"cpu\")[0]\n",
    "        self.clip_model_for_train = self.clip_model_for_train.to(self.device)\n",
    "        if args.ckpt_clip_for_train is not None:\n",
    "            assert os.path.exists(args.ckpt_clip_for_train)\n",
    "            ckpt = torch.load(args.ckpt_clip_for_train)\n",
    "            self.clip_model_for_train.load_state_dict(ckpt[\"model\"])\n",
    "        for p in self.clip_model_for_train.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.clip_model_for_train.eval()\n",
    "\n",
    "        self.clip_visual_size = self.clip_model_for_train.visual.input_resolution\n",
    "\n",
    "        if args.latent_space == \"w\":\n",
    "            output_dim = args.latent\n",
    "        elif args.latent_space == \"wp\":\n",
    "            output_dim = args.latent * self.generator.n_latent\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.sentence2latent = Sentence2DeltaLatent(\n",
    "            args.word_embed_size,\n",
    "            g_latent_dim=args.latent,\n",
    "            out_dim=output_dim,\n",
    "            hidden_dim=args.latent,\n",
    "            num_mlp_layers=args.text_encoder_num_mlp_layers,\n",
    "            return_delta=True,\n",
    "        ).to(self.device)\n",
    "        self.sentence2latent_optimizer = optim.Adam(\n",
    "            self.sentence2latent.parameters(), args.lr\n",
    "        )\n",
    "        self.optimizer_lst = [self.sentence2latent_optimizer]\n",
    "        self.model_lst = [self.sentence2latent]\n",
    "\n",
    "        if args.resume is not None:\n",
    "            print(f\"resume training from {args.resume}\")\n",
    "            ckpt = torch.load(args.resume)\n",
    "            self.start_iter_idx = int(\n",
    "                os.path.splitext(os.path.basename(args.resume))[0]\n",
    "            )\n",
    "            self.sentence2latent.load_state_dict(ckpt[\"sentence_encoder\"])\n",
    "            self.sentence2latent_optimizer.load_state_dict(\n",
    "                ckpt[\"sentence_encoder_optimizer\"]\n",
    "            )\n",
    "        else:\n",
    "            self.start_iter_idx = 0\n",
    "\n",
    "        self.ce_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # dataset\n",
    "        if args.dataset in [\"celebahq\", \"ffhq\"]:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.Resize(args.stylegan_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            train_set = CelebAHQ(\n",
    "                \"data\",\n",
    "                split=\"train\",\n",
    "                transform=transform,\n",
    "            )\n",
    "        elif args.dataset in [\"cub\", \"nabirds\"]:\n",
    "            imsize = args.stylegan_size\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(int(imsize * 76 / 64)),\n",
    "                    transforms.RandomCrop(imsize),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            train_set = CUBZeroShotText(\n",
    "                \"data\",\n",
    "                split=\"train\",\n",
    "                transform=transform,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        collate_fn = pad_text_seq_collate\n",
    "\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            train_set,\n",
    "            args.batch,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "            collate_fn=collate_fn,\n",
    "            persistent_workers=args.num_workers > 0,\n",
    "        )\n",
    "        self.loader = sample_data(self.dataloader)\n",
    "\n",
    "        # for visualization\n",
    "        if args.dataset in [\"celebahq\", \"ffhq\"]:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(args.stylegan_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        elif args.dataset in [\"cub\", \"nabirds\"]:\n",
    "            imsize = args.stylegan_size\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(int(imsize * 76 / 64)),\n",
    "                    transforms.CenterCrop(imsize),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.ckpt_dir = os.path.join(args.exp_dir, \"ckpt\")\n",
    "        if not os.path.exists(self.ckpt_dir):\n",
    "            os.mkdir(self.ckpt_dir)\n",
    "\n",
    "    def zero_grad_all(self):\n",
    "        for o in self.optimizer_lst:\n",
    "            o.zero_grad()\n",
    "\n",
    "    def false_requires_grad_all(self):\n",
    "        for m in self.model_lst:\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def true_requires_grad(self, model_lst):\n",
    "        for m in model_lst:\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_latent(self, noise):\n",
    "        return self.generator(\n",
    "            [noise],\n",
    "            just_latent=True,\n",
    "            truncation=self.args.truncation,\n",
    "            truncation_latent=self.mean_latent,\n",
    "        )[0]\n",
    "\n",
    "    def forward_sentence2latent(\n",
    "        self,\n",
    "        text_embed,\n",
    "        text_len=None,\n",
    "        return_delta=False,\n",
    "        noise=None,\n",
    "        return_rand_latent=False,\n",
    "    ):\n",
    "        if noise is None:\n",
    "            gaussian_noise = torch.randn(\n",
    "                text_embed.shape[0], self.args.latent, device=self.device\n",
    "            )\n",
    "        else:\n",
    "            gaussian_noise = noise\n",
    "        rand_latent = self.get_latent(gaussian_noise)\n",
    "        latent_code, delta = self.sentence2latent(rand_latent, text_embed, text_len)\n",
    "\n",
    "        output_lst = [latent_code]\n",
    "\n",
    "        if return_delta:\n",
    "            output_lst.append(delta)\n",
    "\n",
    "        if return_rand_latent:\n",
    "            output_lst.append(rand_latent)\n",
    "\n",
    "        if len(output_lst) == 1:\n",
    "            return output_lst[0]\n",
    "        else:\n",
    "            return tuple(output_lst)\n",
    "\n",
    "    def g_nonsaturating_loss(self, fake_pred):\n",
    "        loss = F.softplus(-fake_pred).mean()\n",
    "        return loss\n",
    "\n",
    "    def d_logistic_loss(self, real_pred, fake_pred):\n",
    "        real_loss = F.softplus(-real_pred)\n",
    "        fake_loss = F.softplus(fake_pred)\n",
    "\n",
    "        return real_loss.mean() + fake_loss.mean()\n",
    "\n",
    "    def train(self):\n",
    "        log_dict = {}\n",
    "        self.zero_grad_all()\n",
    "        self.false_requires_grad_all()\n",
    "        self.sentence2latent.train()\n",
    "        self.true_requires_grad([self.sentence2latent])\n",
    "\n",
    "        loader_out = next(self.loader)\n",
    "        real_img = loader_out[\"image\"]\n",
    "        clip_tokens = loader_out[\"clip_tokens\"]\n",
    "        text_embed = loader_out[\"word_embeds\"]\n",
    "\n",
    "        text_len = loader_out[\"text_len\"]\n",
    "\n",
    "        real_img = real_img.to(self.device, non_blocking=True)\n",
    "        text_embed = text_embed.to(self.device, non_blocking=True)\n",
    "        clip_tokens = clip_tokens.to(self.device, non_blocking=True)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        latent_code, sentence_delta, rand_latent = self.forward_sentence2latent(\n",
    "            text_embed, text_len, return_delta=True, return_rand_latent=True\n",
    "        )\n",
    "\n",
    "        fake_img = self.generator([latent_code], **self.synthesis_kwargs)[0]\n",
    "        fake_img_for_clip = differentiable_clip_preprocess_from_stylegan(\n",
    "            fake_img, self.clip_visual_size\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            clip_text_feat = self.clip_model_for_train.encode_text(clip_tokens)\n",
    "            clip_text_feat = F.normalize(clip_text_feat, dim=-1)\n",
    "\n",
    "        fake_img_feat = self.clip_model_for_train.encode_image(fake_img_for_clip)\n",
    "        fake_img_feat = F.normalize(fake_img_feat, dim=-1)\n",
    "\n",
    "        logits_per_image_to_text = (\n",
    "            self.clip_model_for_train.logit_scale\n",
    "            * fake_img_feat\n",
    "            @ clip_text_feat.t()\n",
    "        )\n",
    "\n",
    "        ground_truth = torch.arange(\n",
    "            len(logits_per_image_to_text), device=self.device\n",
    "        ).long()\n",
    "        img_text_loss = self.ce_criterion(\n",
    "            logits_per_image_to_text, ground_truth\n",
    "        )\n",
    "\n",
    "        loss += img_text_loss\n",
    "        log_dict[\"clip_fake_img_text_contrastive_loss\"] = img_text_loss.item()\n",
    "\n",
    "        direction_norm = torch.norm(sentence_delta, dim=-1)\n",
    "        threholded_norm = F.relu(\n",
    "            direction_norm - self.args.direction_norm_penalty_threshold\n",
    "        )\n",
    "        threholded_norm = threholded_norm.mean()\n",
    "        threholded_norm = threholded_norm * self.args.lambda_direction_norm_penalty\n",
    "        log_dict[\"direction_norm_loss\"] = threholded_norm.item()\n",
    "        loss += threholded_norm\n",
    "\n",
    "        loss.backward()\n",
    "        self.sentence2latent_optimizer.step()\n",
    "\n",
    "        log_dict[\"loss\"] = loss.item()\n",
    "        return log_dict\n",
    "\n",
    "    def save_checkpoint(self, iteration_idx):\n",
    "        state_dict = {\n",
    "            \"sentence_encoder\": self.sentence2latent.state_dict(),\n",
    "            \"sentence_encoder_optimizer\": self.sentence2latent_optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state_dict, f\"{self.ckpt_dir}/last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNr8rxbSvtCS"
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    name = None\n",
    "    dataset = 'celebahq'\n",
    "    iter = 60001\n",
    "    stylegan_size = 256\n",
    "    lr = 1e-4\n",
    "    batch = 32\n",
    "    num_workers = 12\n",
    "    exp_root = 'exp/stylet2i'\n",
    "    seed = 0\n",
    "    text_encoder_num_mlp_layers = 2      # Initially, it was set to 3\n",
    "    ckpt = 'exp/ckpt/stylegan2_celebahq_size_256_split_train/550000.pt'\n",
    "    resume = None\n",
    "    ckpt_clip_for_train = 'exp/ft_clip_text/ft_clip_ViT-B_32_celebahq_train/ckpt.pth'\n",
    "    truncation = 0.5\n",
    "    latent_space = 'wp'\n",
    "    lambda_direction_norm_penalty = 1.0\n",
    "    direction_norm_penalty_threshold = 8.0\n",
    "    latent = 512\n",
    "    word_embed_size = 300\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1675601404678,
     "user": {
      "displayName": "Liviu Balan",
      "userId": "06381693624622414377"
     },
     "user_tz": -300
    },
    "id": "7VzNYPNfwwoD",
    "outputId": "5f9436f9-b27e-4baa-9b2b-cf8a8cc2beba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4f5421f5b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1028700,
     "status": "ok",
     "timestamp": 1675626381738,
     "user": {
      "displayName": "Liviu Balan",
      "userId": "06381693624622414377"
     },
     "user_tz": -300
    },
    "id": "ZJJf394ew2Re",
    "outputId": "2c2c6a8a-1f14-4af2-fea2-12ff1cdff346"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clip_fake_img_text_contrastive_loss: 2.5826 direction_norm_loss: 0.0059 loss: 2.5885 : 100%|██████████| 60001/60001 [6:56:05<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "name = f\"stylet2i_{args.dataset}_{args.text_encoder_num_mlp_layers}\"\n",
    "if args.name is not None:\n",
    "    name += f\"_{args.name}\"\n",
    "\n",
    "if not os.path.exists(args.exp_root):\n",
    "    os.mkdir(args.exp_root)\n",
    "\n",
    "args.exp_dir = os.path.join(args.exp_root, name)\n",
    "if not os.path.exists(args.exp_dir):\n",
    "    os.mkdir(args.exp_dir)\n",
    "\n",
    "trainer = Trainer(args)\n",
    "\n",
    "pbar = tqdm(range(trainer.start_iter_idx, args.iter), dynamic_ncols=True)\n",
    "for i in pbar:\n",
    "    log_dict = {}\n",
    "    if i % len(trainer.dataloader) == 0:\n",
    "        log_dict[\"epoch\"] = i // len(trainer.dataloader)\n",
    "\n",
    "    if i % 5000 == 0:\n",
    "        trainer.save_checkpoint(i)\n",
    "\n",
    "    train_log_dict = trainer.train()\n",
    "    log_dict.update(train_log_dict)\n",
    "    desc = \"\"\n",
    "    for k, v in log_dict.items():\n",
    "        desc += f\"{k}: {v:.4f} \"\n",
    "    pbar.set_description(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9t2WKgtSm6I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
